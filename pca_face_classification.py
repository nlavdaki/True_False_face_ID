# -*- coding: utf-8 -*-
"""PCA_Face_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oMi2WKJLJqQGfNt7x0sYuuJVcJTWKp7r
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from sklearn.model_selection import train_test_split

# Define your data paths
fake_path = '/content/drive/MyDrive/Colab Notebooks/archive/real_and_fake_face/training_fake/'
real_path = '/content/drive/MyDrive/Colab Notebooks/archive/real_and_fake_face/training_real/'

# Normalization resizing
img_height, img_width = 256, 256  # or 128*128

# Batch size
batch_size = 32

# Function to load & preprocess images
def load_and_preprocess_image(file_path):
    # Load the image as RGB with 3 channels
    img = load_img(file_path, target_size=(img_height, img_width), color_mode='rgb')
    img_array = img_to_array(img)
    img_array /= 255.0  # Normalize pixel values to [0, 1]
    return img_array

# Load & preprocess fake data
fake_files = [os.path.join(fake_path, file) for file in os.listdir(fake_path)]
fake_images = [load_and_preprocess_image(file) for file in fake_files]



# Load & preprocess real data
real_files = [os.path.join(real_path, file) for file in os.listdir(real_path)]
real_images = [load_and_preprocess_image(file) for file in real_files]

fake_labels = np.array([[1, 0]] * len(fake_images))
real_labels = np.array([[0, 1]] * len(real_images))

# Combine fake and real data
X = np.array(fake_images + real_images)
y = np.concatenate((np.zeros(len(fake_images)), np.ones(len(real_images))))

# Shuffle the data
random_indices = np.random.permutation(len(X))
X = X[random_indices]
y = y[random_indices]

# Split the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Ensure that the shape of the data is [batch_size, channels, height, width]
X_train = X_train.transpose(0, 3, 1, 2)  # Transpose the dimensions
X_validation = X_validation.transpose(0, 3, 1, 2)
X_test = X_test.transpose(0, 3, 1, 2)

# Print the number of samples in each set
print(f"Number of samples in the training set: {len(X_train)}")
print(f"Number of samples in the validation set: {len(X_validation)}")
print(f"Number of samples in the test set: {len(X_test)}")

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Define the component and kernel lists
component_list = [90, 95, 100, 105, 110]
kernel_list = ['linear', 'rbf', 'sigmoid']

# Create an empty DataFrame to store results
results_df = pd.DataFrame(columns=['Components', 'Kernel', 'Validation Accuracy', 'Test Accuracy'])

# Iterate over components and kernels
for n_components in component_list:
    for kernel in kernel_list:
        # Perform PCA on the training data
        pca = PCA(n_components=n_components)
        pca.fit(X_train_flattened)

        X_train_pca = pca.transform(X_train_flattened)
        X_validation_pca = pca.transform(X_validation_flattened)
        X_test_pca = pca.transform(X_test_flattened)

        # Choose the classifier (SVM, RandomForest, KNN, etc.) based on the kernel
        if kernel == 'linear':
            clf = SVC(kernel=kernel, random_state=42)
        elif kernel == 'rbf':
            clf = SVC(kernel=kernel, random_state=42)
        elif kernel == 'sigmoid':
            clf = SVC(kernel=kernel, random_state=42)
        # Add other classifiers as needed

        # Train the classifier
        clf.fit(X_train_pca, y_train)

        # Evaluation on validation set
        y_validation_pred = clf.predict(X_validation_pca)
        validation_accuracy = accuracy_score(y_validation, y_validation_pred)

        # Evaluation on test set
        y_test_pred = clf.predict(X_test_pca)
        test_accuracy = accuracy_score(y_test, y_test_pred)

        # Append results to the DataFrame
        results_df = results_df.append({'Components': n_components,
                                        'Kernel': kernel,
                                        'Validation Accuracy': validation_accuracy,
                                        'Test Accuracy': test_accuracy},
                                       ignore_index=True)

# Print the results in tabular form
print(results_df)

from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Step 1: Perform PCA on the training data
n_components = 150  # You can adjust the number of components as needed
pca = PCA(n_components=n_components)


# Flatten the training data to be 2D (batch_size, num_features)
X_train_flattened = X_train.reshape(X_train.shape[0], -1)
X_validation_flattened = X_validation.reshape(X_validation.shape[0], -1)
X_test_flattened = X_test.reshape(X_test.shape[0], -1)

# Fit PCA on the training data
pca.fit(X_train_flattened)

# Transform the training, validation, and test data
X_train_pca = pca.transform(X_train_flattened)
X_validation_pca = pca.transform(X_validation_flattened)
X_test_pca = pca.transform(X_test_flattened)

# Step 2: Train a classifier (SVM is a good choice) on the reduced-dimensional data
clf = SVC(kernel='linear', random_state=42)  # You can choose different kernels if needed

# Train the classifier on the reduced data
clf.fit(X_train_pca, y_train)

# Step 3: Evaluate the model
# Evaluate on the validation set
y_validation_pred = clf.predict(X_validation_pca)
validation_accuracy = accuracy_score(y_validation, y_validation_pred)
print(f"Validation Accuracy: {validation_accuracy * 100:.2f}%")

# Evaluate on the test set
y_test_pred = clf.predict(X_test_pca)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
import pandas as pd
from sklearn.metrics import accuracy_score

component_list = [90, 95, 100, 105, 110]

# Flatten the training data to be 2D (batch_size, num_features)
X_train_flattened = X_train.reshape(X_train.shape[0], -1)
X_validation_flattened = X_validation.reshape(X_validation.shape[0], -1)
X_test_flattened = X_test.reshape(X_test.shape[0], -1)

# Iterate over components and perform Random Forest
for n_components in component_list:
    for _ in range(3):  # Repeat for different random states
        # Perform PCA on the training data
        pca = PCA(n_components=n_components)

        # Fit PCA on the training data
        pca.fit(X_train_flattened)

        # Transform the data
        X_train_pca = pca.transform(X_train_flattened)
        X_validation_pca = pca.transform(X_validation_flattened)
        X_test_pca = pca.transform(X_test_flattened)

        # Random Forest on the reduced-dimensional data
        clf = RandomForestClassifier(random_state=42)

        # Train the classifier
        clf.fit(X_train_pca, y_train)

        # Evaluation on validation set
        y_validation_pred = clf.predict(X_validation_pca)
        validation_accuracy = accuracy_score(y_validation, y_validation_pred)

        # Evaluation on test set
        y_test_pred = clf.predict(X_test_pca)
        test_accuracy = accuracy_score(y_test, y_test_pred)

        # Append results to the DataFrame
        results_df = results_df.append({'Components': n_components,
                                        'Classifier': 'Random Forest',
                                        'Validation Accuracy': validation_accuracy,
                                        'Test Accuracy': test_accuracy},
                                       ignore_index=True)

results_df

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.metrics import accuracy_score

component_list = [90, 95, 100, 105, 110]

# Flatten the training data to be 2D (batch_size, num_features)
X_train_flattened = X_train.reshape(X_train.shape[0], -1)
X_validation_flattened = X_validation.reshape(X_validation.shape[0], -1)
X_test_flattened = X_test.reshape(X_test.shape[0], -1)


# Define an empty DataFrame to store results
results_df = pd.DataFrame(columns=['Components', 'Classifier', 'Validation Accuracy', 'Test Accuracy'])

# Iterate over components and perform Random Forest
for n_components in component_list:
    for _ in range(3):  # Repeat for different random states
        # Perform PCA on the training data
        pca = PCA(n_components=n_components)

        # Fit PCA on the training data
        pca.fit(X_train_flattened)

        # Transform the data
        X_train_pca = pca.transform(X_train_flattened)
        X_validation_pca = pca.transform(X_validation_flattened)
        X_test_pca = pca.transform(X_test_flattened)

        # Random Forest on the reduced-dimensional data
        clf = RandomForestClassifier(random_state=42)

        # Train the classifier
        clf.fit(X_train_pca, y_train)

        # Evaluation on validation set
        y_validation_pred = clf.predict(X_validation_pca)
        validation_accuracy = accuracy_score(y_validation, y_validation_pred)

        # Evaluation on the test set
        y_test_pred = clf.predict(X_test_pca)
        test_accuracy = accuracy_score(y_test, y_test_pred)

        # Append results to the DataFrame
        results_df = results_df.append({'Components': n_components,
                                        'Classifier': 'Random Forest',
                                        'Validation Accuracy': validation_accuracy,
                                        'Test Accuracy': test_accuracy},
                                       ignore_index=True)

results_df

from sklearn.neighbors import KNeighborsClassifier

component_list = [50, 75, 100, 125, 150]  # Adjust as needed

# Flatten the training data to be 2D (batch_size, num_features)
X_train_flattened = X_train.reshape(X_train.shape[0], -1)
X_validation_flattened = X_validation.reshape(X_validation.shape[0], -1)
X_test_flattened = X_test.reshape(X_test.shape[0], -1)


# Define an empty DataFrame to store results
results_df = pd.DataFrame(columns=['Components', 'Classifier', 'Validation Accuracy', 'Test Accuracy'])

# Iterate over components and perform KNN
for n_components in component_list:
    for k in range(1, 6):  # Try different values of k (number of neighbors)
        # Perform PCA on the training data
        pca = PCA(n_components=n_components)

        # Fit PCA on the training data
        pca.fit(X_train_flattened)

        # Transform the data
        X_train_pca = pca.transform(X_train_flattened)
        X_validation_pca = pca.transform(X_validation_flattened)
        X_test_pca = pca.transform(X_test_flattened)

        # KNN on the reduced-dimensional data
        clf = KNeighborsClassifier(n_neighbors=k)

        # Train the classifier
        clf.fit(X_train_pca, y_train)

        # Evaluation on validation set
        y_validation_pred = clf.predict(X_validation_pca)
        validation_accuracy = accuracy_score(y_validation, y_validation_pred)

        # Evaluation on test set
        y_test_pred = clf.predict(X_test_pca)
        test_accuracy = accuracy_score(y_test, y_test_pred)

        # Append results to the DataFrame
        results_df = results_df.append({'Components': n_components,
                                        'Classifier': f'KNN (k={k})',
                                        'Validation Accuracy': validation_accuracy,
                                        'Test Accuracy': test_accuracy},
                                       ignore_index=True)

results_df
