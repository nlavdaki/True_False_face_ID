# -*- coding: utf-8 -*-
"""face_authenticator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/nlavdaki/39a6109993157500a14408eeb0f3070f/face_authenticator.ipynb

https://philarchive.org/archive/SALCOR-3
"""

import matplotlib.pyplot as plt
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

#loading training and validation sets

fake_tr_path = '/content/drive/MyDrive/Colab Notebooks/archive/real_and_fake_face/training_fake/'
real_tr_path= '/content/drive/MyDrive/Colab Notebooks/archive/real_and_fake_face/training_real/'

#Normalization resizing
img_height, img_width = 128, 128  # or 256*256

#Batch size
batch_size = 32

#Function to load & preprocess imgs
def load_and_preprocess_image(file_path):
    #Load
    img = load_img(file_path, target_size=(img_height, img_width))
    #Vectorization
    img_array = img_to_array(img)
    #Normalize pixel values to the range [0, 1]
    img_array /= 255.0
    return img_array

# Load & preprocess training data
fake_tr_files = [os.path.join(fake_tr_path, file) for file in os.listdir(fake_tr_path)]
real_tr_files = [os.path.join(real_tr_path, file) for file in os.listdir(real_tr_path)]

fake_tr_images = [load_and_preprocess_image(file) for file in fake_tr_files]
real_tr_images = [load_and_preprocess_image(file) for file in real_tr_files]

# Labels (0: fake, 1: real)
fake_tr_labels = np.zeros(len(fake_tr_images))
real_tr_labels = np.ones(len(real_tr_images))

# Join fake & real imgs
X_train = np.array(fake_tr_images + real_tr_images)
y_train = np.concatenate((fake_tr_labels, real_tr_labels))

# Shuffle
random_indices = np.random.permutation(len(X_train))
X_train = X_train[random_indices]
y_train = y_train[random_indices]

# Split the data into training, validation, and testing
X_train, X_temp, y_train, y_temp = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Data
# Augmenting
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')
validation_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Training generator
train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)

# Validation generator
validation_generator = validation_datagen.flow(X_validation, y_validation, batch_size=batch_size)

# Testing generator
test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)

import matplotlib.pyplot as plt

#Function for grid of images
def plot_images(images, titles, rows, cols):
    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i])
        ax.set_title(titles[i])
        ax.axis('off')

#Subset of images to visualize
subset_images = X_train[:9]
subset_labels = y_train[:9]

#Convert labels
subset_labels = ["Fake" if label == 0 else "Real" for label in subset_labels]

plot_images(subset_images, subset_labels, 3, 3)
plt.show()

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split

# Input shape
input_shape = (img_height, img_width, 3)  #RGB images

num_classes = 2  #(fake and real)

#CNN model
def build_cnn_model():
    # Define the model
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation='relu'),  #Hidden layer
        tf.keras.layers.Dense(num_classes, activation='softmax')  #Output layer
    ])

    return model

#Build the CNN model
model = build_cnn_model()

#Compile the model
learning_rate = 0.0001
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)

# Train the model
epochs = 150
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator,
    callbacks=[early_stopping]
)

# Evaluate on the validation set
validation_loss, validation_accuracy = model.evaluate(validation_generator)

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(test_generator)

print(f"Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}")
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

from sklearn.metrics import confusion_matrix
import seaborn as sns

#Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training & validation accuracy values
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Predict the test set
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Create a confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns

#Input shape
input_shape = (img_height, img_width, 3)

num_classes = 2

#ResNet50 model as base
base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))

#Custom layers on top of ResNet50
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)  #Hidden layer
predictions = Dense(num_classes, activation='softmax')(x)  #Output layer

#Model creation
model = Model(inputs=base_model.input, outputs=predictions)

#Freeze the layers of the base model
for layer in base_model.layers:
    layer.trainable = False

#Compile the model
learning_rate = 0.0001
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)

#Train
epochs = 10  # You can adjust the number of epochs
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator,
    callbacks=[early_stopping]
)

# Evaluate on the validation set
validation_loss, validation_accuracy = model.evaluate(validation_generator)

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(test_generator)

print(f"Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}")
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

#Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Plot training & validation accuracy values
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Predict the test set
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

#Create a confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred_classes)

#Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import InceptionV3

# Input shape
input_shape = (img_height, img_width, 3)

num_classes = 2

#InceptionV3 model as base
base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))

# Custom layers on top of InceptionV3
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)  # Hidden layer
x = Dropout(0.5)(x)  # Dropout layer for regularization
predictions = Dense(num_classes, activation='softmax')(x)  # Output layer

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the layers of the base model
for layer in base_model.layers:
    layer.trainable = False

# Compile the model
learning_rate = 0.0001
optimizer = Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
epochs = 15  # You can adjust the number of epochs
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator,
    callbacks=[early_stopping]
)

# Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot training & validation accuracy values
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Predict the test set
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)

# Create a confusion matrix
confusion_mtx = confusion_matrix(y_test, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()